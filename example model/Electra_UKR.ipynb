{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"a1AdP7YlpgMi"},"outputs":[],"source":["!pip install sentence_transformers\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFw2tJmvqQrL"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","import regex as re\n","from keras.layers import Dropout, Conv1D, Flatten, Dense, MaxPooling1D, LSTM, Bidirectional, GlobalAveragePooling1D\n","from transformers import AutoTokenizer, TFAutoModel, AutoModelWithLMHead"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zreDA6g0pidc"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BiSlAtTTVYyg"},"outputs":[],"source":["# Seed value\n","# Apparently you may use different seed values at each stage\n","seed_value= 42\n","\n","# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n","import os\n","os.environ['PYTHONHASHSEED']=str(seed_value)\n","\n","# 2. Set the `python` built-in pseudo-random generator at a fixed value\n","import random\n","random.seed(seed_value)\n","\n","# 3. Set the `numpy` pseudo-random generator at a fixed value\n","np.random.seed(seed_value)\n","\n","# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n","tf.random.set_seed(seed_value)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QrZgSlY7qaC3"},"outputs":[],"source":["tweetData = pd.read_csv(\"/content/drive/MyDrive/train_small.csv\", encoding=\"utf-8\")\n","tweetData"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1o31alJMliNZ"},"outputs":[],"source":["def clean_text(text):\n","    text = re.sub(r'\\n', ' ', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'#\\w+', '', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","    text = text.strip()\n","    text = re.sub(r'\\s+([?.!,\"])', r'\\1', text)\n","    text = re.sub(r'[^\\w\\s]', '', text).lower()\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TQdzNVh_llM8"},"outputs":[],"source":["get_text = tweetData[\"text\"]\n","clean_text = list(map(clean_text, get_text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ATt88J1NYqqy"},"outputs":[],"source":["X = clean_text\n","y = np.asarray(tweetData.source)\n","y_one_hot = tf.keras.utils.to_categorical(y-1, num_classes = 7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69i797ju55Qq"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.3, random_state=42)\n","X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.15, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6H_digUgSNU"},"outputs":[],"source":["MODEL_NAME = \"lang-uk/electra-base-ukrainian-cased-discriminator\"\n","MAX_LEN = 512\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AjhMIoP2L451"},"outputs":[],"source":["def encode(texts, tokenizer):\n","    ct = len(texts)\n","    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n","    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n","\n","    for k, text in enumerate(texts):\n","        # Tokenize\n","        tok_text = tokenizer.tokenize(text)\n","        \n","        # Truncate and convert tokens to numerical IDs\n","        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])\n","        \n","        input_length = len(enc_text) + 2\n","        input_length = input_length if input_length < MAX_LEN else MAX_LEN\n","        \n","        # Add tokens [CLS] and [SEP] at the beginning and the end\n","        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n","        \n","        # Set to 1s in the attention input\n","        attention_mask[k,:input_length] = 1\n","\n","\n","    return {\n","        'input_word_ids': input_ids,\n","        'input_mask': attention_mask,\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F20F-ThJ_EIz"},"outputs":[],"source":["X_train = encode(X_train, tokenizer)\n","X_test = encode(X_test, tokenizer)\n","X_val = encode(X_val, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uIXmLE8dcvsl"},"outputs":[],"source":["\"\"\"\n","========================================\n","Electra LINEAR\n","========================================\n","\"\"\"\n","def build_model1():\n","  input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n","  input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n","\n","  trf_model = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)\n","  x = trf_model(input_word_ids, attention_mask=input_mask)\n","  x = x[0]\n","\n","  x = Flatten()(x)\n","  x = Dense(128, activation=\"relu\")(x)\n","  x = Dense(7, activation='softmax')(x)\n","\n","  model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=x)\n","  model.compile(\n","      optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n","      loss= tf.keras.losses.binary_crossentropy,\n","      metrics=['accuracy'])\n","  return model\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"12LC9wuLtqno"},"outputs":[],"source":["\"\"\"\n","========================================\n","Roberta + CNN \n","batch = 16\n","epochs = 3\n","========================================\n","\"\"\"\n","\n","def build_model2(conv_size=128):\n","  input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n","  input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n","\n","  trf_model = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)\n","  x = trf_model(input_word_ids, attention_mask=input_mask)\n","  x = x[0]\n","\n","  x = Conv1D(conv_size, 3, padding='same', activation='relu')(x)\n","  x = Conv1D(conv_size, 2, padding='same', activation=\"relu\")(x)\n","  x = MaxPooling1D(pool_size=5, strides=2, padding=\"same\")(x)\n","  x = Flatten()(x)\n","  \n","  x = Dense(128, activation='relu')(x)\n","  x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n","\n","  model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=x)\n","  model.compile(\n","      optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n","      loss= tf.keras.losses.binary_crossentropy,\n","      metrics=['accuracy'])\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MwwqtHF_mnqK"},"outputs":[],"source":["\"\"\"\n","========================================\n","Electra + LSTM\n","========================================\n","\"\"\"\n","\n","def build_model3():\n","  input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n","  input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n","\n","  trf_model = TFAutoModel.from_pretrained(MODEL_NAME, from_pt=True)\n","  x = trf_model(input_word_ids, attention_mask=input_mask)\n","  x = x[0]\n","\n","  x = Bidirectional(LSTM(256, return_sequences=True))(x)\n","  x = Bidirectional(LSTM(128, return_sequences=True))(x)\n","  x = MaxPooling1D(pool_size=5, strides=2, padding=\"same\")(x)\n","  x = Flatten()(x)\n","  x = Dense(64, activation='relu')(x)\n","  x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n","\n","  model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=x)\n","  model.compile(\n","      optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5),\n","      loss= tf.keras.losses.binary_crossentropy,\n","      metrics=['accuracy'])\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMgrTrJ0lcZX"},"outputs":[],"source":["model = build_model1()\n","history = model.fit(X_train,\n","                    y_train,\n","                    epochs=3,\n","                    batch_size=16,\n","                    verbose=1,\n","                    validation_data=(X_val, y_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mABXRa8ksPfv"},"outputs":[],"source":["y_pred = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBbtduMCsQU9"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RbqxYogiNyKC"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","con_mat_df = confusion_matrix(y_test, y_pred)\n","cmn = con_mat_df.astype('float') / con_mat_df.sum(axis=1)[:, np.newaxis]\n","disp = ConfusionMatrixDisplay(confusion_matrix=cmn, display_labels=[\"neutral\", \"hate\"])\n","\n","disp.plot(cmap=plt.cm.Blues)\n","plt.title(\"Roberta Model\")\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","private_outputs":true,"provenance":[{"file_id":"13RPeX04TExnmPbQ6GEQOTCl_J5b_vjPL","timestamp":1681818646806},{"file_id":"1Yw-mr4z9C_jRWzCQzQ_36Tct6E8TylUc","timestamp":1681809383241},{"file_id":"11RIHhrzAI2qLvBJxakA1CRTcw4BD28Fi","timestamp":1681736037988},{"file_id":"https://github.com/DaryaTereshchenko/HateSpeechDetection/blob/main/RoBerta_all_moodels.ipynb","timestamp":1678809878349}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
